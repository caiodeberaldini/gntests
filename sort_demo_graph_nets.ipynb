{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.demos_tf2 import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dicts_tf(num_examples, num_elements_min_max):\n",
    "    \"\"\"\n",
    "    Generate graphs for training.\n",
    "\n",
    "    Args:\n",
    "    num_examples: total number of graphs to generate\n",
    "    num_elements_min_max: a 2-tuple with the minimum and maximum number of\n",
    "      values allowable in a graph. The number of values for a graph is\n",
    "      uniformly sampled withing this range. The upper bound is exclusive, and\n",
    "      should be at least 2 more than the lower bound.\n",
    "\n",
    "    Returns:\n",
    "    inputs: contains the generated random numbers as node values.\n",
    "    sort_indices: contains the sorting indices as nodes. Concretely\n",
    "      inputs.nodes[sort_indices.nodes] will be a sorted array.\n",
    "    ranks: the rank of each value in inputs normalized to the range [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    num_elements = tf.random.uniform(\n",
    "      [num_examples],\n",
    "      minval=num_elements_min_max[0],\n",
    "      maxval=num_elements_min_max[1],\n",
    "      dtype=tf.int32)\n",
    "    inputs_graphs = []\n",
    "    sort_indices_graphs = []\n",
    "    ranks_graphs = []\n",
    "    for i in range(num_examples):\n",
    "        values = tf.random.uniform(shape=[num_elements[i]])\n",
    "        sort_indices = tf.cast(\n",
    "            tf.argsort(values, axis=-1), tf.float32)\n",
    "        ranks = tf.cast(\n",
    "            tf.argsort(sort_indices, axis=-1), tf.float32) / (\n",
    "                tf.cast(num_elements[i], tf.float32) - 1.0)\n",
    "        inputs_graphs.append({\"nodes\": values[:, None]})\n",
    "        sort_indices_graphs.append({\"nodes\": sort_indices[:, None]})\n",
    "        ranks_graphs.append({\"nodes\": ranks[:, None]})\n",
    "\n",
    "    return inputs_graphs, sort_indices_graphs, ranks_graphs\n",
    "\n",
    "\n",
    "def create_linked_list_target(batch_size, input_graphs):\n",
    "    \"\"\"\n",
    "    Creates linked list targets.\n",
    "\n",
    "    Returns a graph with the same number of nodes as `input_graph`. Each node\n",
    "    contains a 2d vector with targets for a 1-class classification where only one\n",
    "    node is `True`, the smallest value in the array. The vector contains two\n",
    "    values: [prob_true, prob_false].\n",
    "    It also contains edges connecting all nodes. These are again 2d vectors with\n",
    "    softmax targets [prob_true, prob_false]. An edge is True\n",
    "    if n+1 is the element immediately after n in the sorted list.\n",
    "\n",
    "    Args:\n",
    "    batch_size: batch size for the `input_graphs`.\n",
    "    input_graphs: a `graphs.GraphsTuple` which contains a batch of inputs.\n",
    "\n",
    "    Returns:\n",
    "    A `graphs.GraphsTuple` with the targets, which encode the linked list.\n",
    "    \"\"\"\n",
    "\n",
    "    target_graphs = []\n",
    "    for i in range(batch_size):\n",
    "        input_graph = utils_tf.get_graph(input_graphs, i)\n",
    "        num_elements = tf.shape(input_graph.nodes)[0]\n",
    "        si = tf.cast(tf.squeeze(input_graph.nodes), tf.int32)\n",
    "        nodes = tf.reshape(tf.one_hot(si[:1], num_elements), (-1, 1))\n",
    "        x = tf.stack((si[:-1], si[1:]))[None]\n",
    "        y = tf.stack(\n",
    "            (input_graph.senders, input_graph.receivers), axis=1)[:, :, None]\n",
    "        edges = tf.reshape(\n",
    "            tf.cast(\n",
    "                tf.reduce_any(tf.reduce_all(tf.equal(x, y), axis=1), axis=1),\n",
    "                tf.float32), (-1, 1))\n",
    "        target_graphs.append(input_graph._replace(nodes=nodes, edges=edges))\n",
    "\n",
    "    return utils_tf.concat(target_graphs, axis=0)\n",
    "\n",
    "\n",
    "def compute_accuracy(target, output):\n",
    "    \"\"\"\n",
    "    Calculate model accuracy.\n",
    "\n",
    "    Returns the number of correctly predicted links and the number\n",
    "    of completely solved list sorts (100% correct predictions).\n",
    "\n",
    "    Args:\n",
    "    target: A `graphs.GraphsTuple` that contains the target graph.\n",
    "    output: A `graphs.GraphsTuple` that contains the output graph.\n",
    "\n",
    "    Returns:\n",
    "    correct: A `float` fraction of correctly labeled nodes/edges.\n",
    "    solved: A `float` fraction of graphs that are completely correctly labeled.\n",
    "    \"\"\"\n",
    "\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    cs = []\n",
    "    ss = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        num_elements = td[\"nodes\"].shape[0]\n",
    "        xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "        yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "\n",
    "        xe = np.reshape(\n",
    "            np.argmax(\n",
    "                np.reshape(td[\"edges\"], (num_elements, num_elements, 2)), axis=-1),\n",
    "            (-1,))\n",
    "        ye = np.reshape(\n",
    "            np.argmax(\n",
    "                np.reshape(od[\"edges\"], (num_elements, num_elements, 2)), axis=-1),\n",
    "            (-1,))\n",
    "        c = np.concatenate((xn == yn, xe == ye), axis=0)\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "\n",
    "    return correct, solved\n",
    "\n",
    "\n",
    "def create_data(batch_size, num_elements_min_max):\n",
    "    \"\"\"\n",
    "    Returns graphs containing the inputs and targets for classification.\n",
    "\n",
    "    Refer to create_data_dicts_tf and create_linked_list_target for more details.\n",
    "\n",
    "    Args:\n",
    "    batch_size: batch size for the `input_graphs`.\n",
    "    num_elements_min_max: a 2-`tuple` of `int`s which define the [lower, upper)\n",
    "      range of the number of elements per list.\n",
    "\n",
    "    Returns:\n",
    "    inputs: a `graphs.GraphsTuple` which contains the input list as a graph.\n",
    "    targets: a `graphs.GraphsTuple` which contains the target as a graph.\n",
    "    sort_indices: a `graphs.GraphsTuple` which contains the sort indices of\n",
    "      the list elements a graph.\n",
    "    ranks: a `graphs.GraphsTuple` which contains the ranks of the list\n",
    "      elements as a graph.\n",
    "  \"\"\"\n",
    "\n",
    "    inputs, sort_indices, ranks = create_graph_dicts_tf(\n",
    "      batch_size, num_elements_min_max)\n",
    "    inputs = utils_tf.data_dicts_to_graphs_tuple(inputs)\n",
    "    sort_indices = utils_tf.data_dicts_to_graphs_tuple(sort_indices)\n",
    "    ranks = utils_tf.data_dicts_to_graphs_tuple(ranks)\n",
    "\n",
    "    inputs = utils_tf.fully_connect_graph_dynamic(inputs)\n",
    "    sort_indices = utils_tf.fully_connect_graph_dynamic(sort_indices)\n",
    "    ranks = utils_tf.fully_connect_graph_dynamic(ranks)\n",
    "\n",
    "    targets = create_linked_list_target(batch_size, sort_indices)\n",
    "    nodes = tf.concat((targets.nodes, 1.0 - targets.nodes), axis=1)\n",
    "    edges = tf.concat((targets.edges, 1.0 - targets.edges), axis=1)\n",
    "    targets = targets._replace(nodes=nodes, edges=edges)\n",
    "\n",
    "    return inputs, targets, sort_indices, ranks\n",
    "\n",
    "\n",
    "def create_loss(target, outputs):\n",
    "    \"\"\"\n",
    "    Returns graphs containing the inputs and targets for classification.\n",
    "\n",
    "    Refer to create_data_dicts_tf and create_linked_list_target for more details.\n",
    "\n",
    "    Args:\n",
    "    target: a `graphs.GraphsTuple` which contains the target as a graph.\n",
    "    outputs: a `list` of `graphs.GraphsTuple`s which contains the model\n",
    "      outputs for each processing step as graphs.\n",
    "\n",
    "    Returns:\n",
    "    A `list` of ops which are the loss for each processing step.\n",
    "    \"\"\"\n",
    "\n",
    "    # if not isinstance(outputs, collections.Sequence):\n",
    "    #   outputs = [outputs]\n",
    "    losss = [\n",
    "      tf.compat.v1.losses.softmax_cross_entropy(target.nodes, output.nodes) +\n",
    "      tf.compat.v1.losses.softmax_cross_entropy(target.edges, output.edges)\n",
    "      for output in outputs\n",
    "    ]\n",
    "    return tf.stack(losss)\n",
    "\n",
    "\n",
    "\n",
    "def plot_linked_list(ax, graph, sort_indices):\n",
    "    \"\"\"\n",
    "    Plot a networkx graph containing weights \n",
    "    for the linked list probability.\n",
    "    \"\"\"\n",
    "\n",
    "    nd = len(graph.nodes())\n",
    "    probs = np.zeros((nd, nd))\n",
    "    for edge in graph.edges(data=True):\n",
    "        probs[edge[0], edge[1]] = edge[2][\"features\"][0]\n",
    "    ax.matshow(probs[sort_indices][:, sort_indices], cmap=\"viridis\")\n",
    "    ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\caio\\gnenv\\lib\\site-packages\\graph_nets\\utils_tf.py:793: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_elements_min_max = (5, 10)\n",
    "\n",
    "inputs, targets, sort_indices, ranks = create_data(\n",
    "    1, num_elements_min_max)\n",
    "\n",
    "inputs_nodes = inputs.nodes.numpy()\n",
    "targets = utils_tf.nest_to_numpy(targets)\n",
    "sort_indices_nodes = sort_indices.nodes.numpy()\n",
    "ranks_nodes = ranks.nodes.numpy()\n",
    "\n",
    "sort_indices = np.squeeze(sort_indices_nodes).astype(int)\n",
    "\n",
    "# Plot sort linked lists.\n",
    "# The matrix plots show each element from the sorted list (rows), and which\n",
    "# element they link to as next largest (columns). Ground truth is a diagonal\n",
    "# offset toward the upper-right by one.\n",
    "fig = plt.figure(1, figsize=(4, 4))\n",
    "fig.clf()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plot_linked_list(ax,\n",
    "                 utils_np.graphs_tuple_to_networkxs(targets)[0], sort_indices)\n",
    "ax.set_title(\"Element-to-element links for sorted elements\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "fig = plt.figure(2, figsize=(10, 2))\n",
    "fig.clf()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "i = 0\n",
    "num_elements = ranks_nodes.shape[0]\n",
    "inputs = np.squeeze(inputs_nodes)\n",
    "ranks = np.squeeze(ranks_nodes * (num_elements - 1.0)).astype(int)\n",
    "x = np.arange(inputs.shape[0])\n",
    "\n",
    "ax1.set_title(\"Inputs\")\n",
    "ax1.barh(x, inputs, color=\"b\")\n",
    "ax1.set_xlim(-0.01, 1.01)\n",
    "\n",
    "ax2.set_title(\"Sorted\")\n",
    "ax2.barh(x, inputs[sort_indices], color=\"k\")\n",
    "ax2.set_xlim(-0.01, 1.01)\n",
    "\n",
    "ax3.set_title(\"Ranks\")\n",
    "ax3.barh(x, ranks, color=\"r\")\n",
    "_ = ax3.set_xlim(0, len(ranks) + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we explore includes three components:\n",
    "# - An \"Encoder\" graph net, which independently encodes the edge, node, and\n",
    "#   global attributes (does not compute relations etc.).\n",
    "# - A \"Core\" graph net, which performs N rounds of processing (message-passing)\n",
    "#   steps. The input to the Core is the concatenation of the Encoder's output\n",
    "#   and the previous output of the Core (labeled \"Hidden(t)\" below, where \"t\" is\n",
    "#   the processing step).\n",
    "# - A \"Decoder\" graph net, which independently decodes the edge, node, and\n",
    "#   global attributes (does not compute relations etc.), on each\n",
    "#   message-passing step.\n",
    "#\n",
    "#                     Hidden(t)   Hidden(t+1)\n",
    "#                        |            ^\n",
    "#           *---------*  |  *------*  |  *---------*\n",
    "#           |         |  |  |      |  |  |         |\n",
    "# Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)\n",
    "#           |         |---->|      |     |         |\n",
    "#           *---------*     *------*     *---------*\n",
    "#\n",
    "# The model is trained by supervised learning. Input graphs are procedurally\n",
    "# generated, and output graphs have the same structure with the nodes and edges\n",
    "# of the linked list labeled (using 2-element 1-hot vectors). The target\n",
    "# labels the node corresponding to the lowest value in the list, and labels each\n",
    "# which represents the connection between neighboring values in the sorted\n",
    "# list.\n",
    "#\n",
    "# The training loss is computed on the output of each processing step. The\n",
    "# reason for this is to encourage the model to try to solve the problem in as\n",
    "# few steps as possible. It also helps make the output of intermediate steps\n",
    "# more interpretable.\n",
    "#\n",
    "# There's no need for a separate evaluate dataset because the inputs are\n",
    "# never repeated, so the training loss is the measure of performance on graphs\n",
    "# from the input distribution.\n",
    "#\n",
    "# We also evaluate how well the models generalize to lists which are up to\n",
    "# twice as large as those on which it was trained. The loss is computed only\n",
    "# on the final processing step.\n",
    "#\n",
    "# Variables with the suffix _tr are training parameters, and variables with the\n",
    "# suffix _ge are test/generalization parameters.\n",
    "#\n",
    "# After around 2000-5000 training iterations the model reaches near-perfect\n",
    "# performance on lists with between 8-16 elements.\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "# Data / training parameters.\n",
    "num_training_iterations = 3000\n",
    "batch_size_tr = 32\n",
    "batch_size_ge = 100\n",
    "# Number of elements in each list is sampled uniformly from this range.\n",
    "num_elements_min_max_tr = (8, 17)\n",
    "num_elements_min_max_ge = (16, 33)\n",
    "\n",
    "# Data.\n",
    "@tf.function\n",
    "def get_data():\n",
    "    inputs_tr, targets_tr, sort_indices_tr, _ = create_data(\n",
    "      batch_size_tr, num_elements_min_max_tr)\n",
    "    inputs_tr = utils_tf.set_zero_edge_features(inputs_tr, 1)\n",
    "    inputs_tr = utils_tf.set_zero_global_features(inputs_tr, 1)\n",
    "    # Test/generalization.\n",
    "    inputs_ge, targets_ge, sort_indices_ge, _ = create_data(\n",
    "      batch_size_ge, num_elements_min_max_ge)\n",
    "    inputs_ge = utils_tf.set_zero_edge_features(inputs_ge, 1)\n",
    "    inputs_ge = utils_tf.set_zero_global_features(inputs_ge, 1)\n",
    "\n",
    "    targets_tr = utils_tf.set_zero_global_features(targets_tr, 1)\n",
    "    targets_ge = utils_tf.set_zero_global_features(targets_ge, 1)\n",
    "\n",
    "    return inputs_tr, targets_tr, sort_indices_tr, inputs_ge, targets_ge, sort_indices_ge\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []\n",
    "\n",
    "\n",
    "# Training.\n",
    "def update_step(inputs_tr, targets_tr):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model(inputs_tr, num_processing_steps_tr)\n",
    "        # Loss.\n",
    "        loss_tr = create_loss(targets_tr, outputs_tr)\n",
    "        loss_tr = tf.math.reduce_sum(loss_tr) / num_processing_steps_tr\n",
    "\n",
    "    gradients = tape.gradient(loss_tr, model.trainable_variables)\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    return outputs_tr, loss_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To recover the speed of TensorFlow 1, we need to use `tf.function` to\n",
    "compile the update_step into a graph. However, using `tf.function` naively\n",
    "may futher reduce performance as the number of nodes/edges in the batch can\n",
    "change across batches, tensorflow will repeatedly trace the function multiple\n",
    "times for each unique shape of the input tensors.\n",
    "Instead, we obtain an explicit signature for each input argument to\n",
    "`update_step` using `utils_tf.specs_from_graphs_tuple` that sets `None` sizes\n",
    "for variable length axes in graph fields, preventing `tf.function` from having\n",
    "to trace the function repeatedly.\n",
    "\"\"\"\n",
    "\n",
    "# Get some example data that resembles the tensors that will be fed\n",
    "# into update_step():\n",
    "example_input_data, example_target_data = get_data()[:2]\n",
    "\n",
    "# Get the input signature for that function by obtaining the specs\n",
    "input_signature = [\n",
    "  utils_tf.specs_from_graphs_tuple(example_input_data),\n",
    "  utils_tf.specs_from_graphs_tuple(example_target_data)\n",
    "]\n",
    "\n",
    "# Compile the update function using the input signature for speedy code.\n",
    "compiled_update_step = tf.function(update_step, input_signature=input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# Instantiate the model.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "    (inputs_tr, targets_tr, sort_indices_tr,\n",
    "    inputs_ge, targets_ge, sort_indices_ge) = get_data()\n",
    "\n",
    "    outputs_tr, loss_tr = compiled_update_step(inputs_tr, targets_tr)\n",
    "\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        last_log_time = the_time\n",
    "        outputs_ge = model(inputs_ge, num_processing_steps_ge)\n",
    "        losss_ge = create_loss(targets_ge, outputs_ge)\n",
    "        loss_ge = losss_ge[-1]\n",
    "\n",
    "        # Replace the globals again to prevent exceptions.\n",
    "        outputs_tr[-1] = outputs_tr[-1].replace(globals=None)\n",
    "        targets_tr = targets_tr.replace(globals=None)\n",
    "\n",
    "        correct_tr, solved_tr = compute_accuracy(\n",
    "            utils_tf.nest_to_numpy(targets_tr),\n",
    "            utils_tf.nest_to_numpy(outputs_tr[-1]))\n",
    "        correct_ge, solved_ge = compute_accuracy(\n",
    "            utils_tf.nest_to_numpy(targets_ge),\n",
    "            utils_tf.nest_to_numpy(outputs_ge[-1]))\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(loss_tr.numpy())\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(loss_ge.numpy())\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "        print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, \"\n",
    "              \"Str {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(\n",
    "                  iteration, elapsed, loss_tr.numpy(), loss_ge.numpy(),\n",
    "                  correct_tr, solved_tr, correct_ge, solved_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell visualizes the results of training. You can visualize the\n",
    "# intermediate results by interrupting execution of the cell above, and running\n",
    "# this cell. You can then resume training by simply executing the above cell\n",
    "# again.\n",
    "\n",
    "# Plot results curves.\n",
    "fig = plt.figure(11, figsize=(18, 3))\n",
    "fig.clf()\n",
    "x = np.array(logged_iterations)\n",
    "# Loss.\n",
    "y_tr = losses_tr\n",
    "y_ge = losses_ge\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Loss across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Loss (binary cross-entropy)\")\n",
    "ax.legend()\n",
    "# Correct.\n",
    "y_tr = corrects_tr\n",
    "y_ge = corrects_ge\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Fraction correct across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Fraction nodes/edges correct\")\n",
    "# Solved.\n",
    "y_tr = solveds_tr\n",
    "y_ge = solveds_ge\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Fraction solved across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Fraction examples solved\")\n",
    "\n",
    "# Plot sort linked lists for test/generalization.\n",
    "# The matrix plots show each element from the sorted list (rows), and which\n",
    "# element they link to as next largest (columns). Ground truth is a diagonal\n",
    "# offset toward the upper-right by one.\n",
    "outputs = utils_np.graphs_tuple_to_networkxs(outputs_tr[-1])\n",
    "targets = utils_np.graphs_tuple_to_networkxs(targets_tr)\n",
    "inputs = utils_np.graphs_tuple_to_networkxs(inputs_tr)\n",
    "batch_element = 0\n",
    "fig = plt.figure(12, figsize=(8, 4.5))\n",
    "fig.clf()\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "sort_indices = np.squeeze(\n",
    "    utils_np.get_graph(sort_indices_tr,\n",
    "                       batch_element).nodes).astype(int)\n",
    "fig.suptitle(\"Element-to-element link predictions for sorted elements\")\n",
    "plot_linked_list(ax1, targets[batch_element], sort_indices)\n",
    "ax1.set_title(\"Ground truth\")\n",
    "ax1.set_axis_off()\n",
    "plot_linked_list(ax2, outputs[batch_element], sort_indices)\n",
    "ax2.set_title(\"Predicted\")\n",
    "ax2.set_axis_off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
